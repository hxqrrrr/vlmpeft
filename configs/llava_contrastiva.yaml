model_name: "openai/clip-vit-base-patch32"
model_type: "CLIP"
lora_rank: 16
task_type: "contrastive"
data_path: "data/raw/"
batch_size: 16
learning_rate: 1e-4
num_epochs: 5
log_interval: 50
checkpoint_dir: "data/checkpoints/"